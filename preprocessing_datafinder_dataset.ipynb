{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g23tka04/miniconda3/envs/test1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import re \n",
    "from re import split\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('./datafinder/datafinder_dataset/train_data.jsonl', lines=True)\n",
    "\n",
    "df_dataset_information = pd.read_json('./datafinder/datafinder_dataset/dataset_search_collection.jsonl', lines=True)\n",
    "df.drop_duplicates(subset=['paper_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = (\n",
    "    r'\\\\\\(.*?\\\\\\)|'  # LaTeX inline math (\\(...\\))\n",
    "    r'\\r\\n|'  # Line breaks\n",
    "    r'\\*\\*|'  # Asterisks\n",
    "    r'\\$.*?\\$|'  # LaTeX inline math ($...$)\n",
    "    r'\\\\\\[.*?\\\\\\]|'  # LaTeX display math (\\[...\\])\n",
    "    r'https?://\\S+|'  # URLs starting with http:// or https://\n",
    "    r'www\\.\\S+|'  # URLs starting with www.\n",
    "    r'ftp://\\S+|'  # URLs starting with ftp://\n",
    "    r'\\\\begin\\{equation\\}.*?\\\\end\\{equation\\}|'  # LaTeX display math (\\begin{equation}...\\end{equation})\n",
    "    r'\\\\[a-zA-Z]+\\*?(?:\\[[^\\]]*\\])?(?:\\{[^}]*\\})?|' # LaTeX commands\n",
    "    r'\\\\langle.*?\\\\rangle|'  # LaTeX angle brackets (\\langle...\\rangle)\n",
    "    r'https?://[^\\s]+(?:[\\s\\.,]|$)|'  # Match http or https URLs, followed by space, dot, or end of string\n",
    "    r'www\\.[^\\s]+(?:[\\s\\.,]|$)'  # Match URLs starting with www., followed by space, dot, or end of string\n",
    "    r'\\[Image Source\\: \\[.*?\\]|'\n",
    "    r'\\(Image Source\\: \\[|'\n",
    "    r'\\[Image Source\\: \\[|'\n",
    "    r'\\(Source\\: \\[.*?\\]|'\n",
    "    r'Source\\: \\[|'  \n",
    "    r'\\(\\s*/paper/[^)]+\\s*\\)'\n",
    "    \n",
    ")  \n",
    "\n",
    "df['abstract'] = df['abstract'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# df_dataset_information['contents'] = df_dataset_information['contents'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "df_graph = df[['paper_id', 'outbound_citations', 'positives', 'negatives']]\n",
    "\n",
    "df_paper = df[['title', 'abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17397\n"
     ]
    }
   ],
   "source": [
    "pprIdx = {paper_id: idx for idx, paper_id in enumerate(df['paper_id'])}\n",
    "\n",
    "print(len(pprIdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CIFAR-10': 0, 'ImageNet': 1, 'CAT2000': 2, 'Hopkins155': 3, 'VRD': 4, 'Middlebury': 5, 'KITTI': 6, 'LAMA': 7, 'COCO': 8, 'Jester': 9, 'JFLEG': 10, 'IAM': 11, 'Set5': 12, 'AFW': 13, 'FRGC': 14, 'WikiBio': 15, 'Cora': 16, 'BSDS500': 17, 'MovieLens': 18, 'COMA': 19, 'UCF101': 20, 'CARLA': 21, 'Birdsnap': 22, 'PACS': 23, 'VCR': 24, 'ARC': 25, 'MultiRC': 26, 'AudioSet': 27, 'Flickr30k': 28, 'CelebA': 29, 'MuJoCo': 30, 'ReferItGame': 31, 'WebText': 32, 'WikiText-103': 33, 'DeepFashion': 34, 'DRCD': 35, 'NewsQA': 36, 'SQuAD': 37, 'WN18': 38, 'AFLW': 39, 'Helen': 40, 'SumMe': 41, 'Django': 42, 'CompCars': 43, 'ETH': 44, 'Caltech-101': 45, 'LFPW': 46, 'Cityscapes': 47, 'RaFD': 48, 'ECSSD': 49, 'SimpleQuestions': 50, 'QNLI': 51, 'MRPC': 52, 'RACE': 53, 'GLUE': 54, 'SNLI': 55, 'DRIVE': 56, 'ShapeNet': 57, 'MultiNLI': 58, 'DAQUAR': 59, 'IEMOCAP': 60, 'VIPeR': 61, 'STARE': 62, 'ShanghaiTech': 63, 'Pix3D': 64, 'CAD-120': 65, 'HandNet': 66, 'CoQA': 67, 'HMDB51': 68, 'FaceForensics': 69, 'DAVIS': 70, 'ActivityNet': 71, 'BioGRID': 72, 'DBpedia': 73, 'ZINC': 74, 'TriviaQA': 75, 'MPII': 76, 'VizDoom': 77, 'ESC-50': 78, 'ORL': 79, 'McMaster': 80, 'DTD': 81, 'SYNTHIA': 82, 'ModelNet': 83, 'SST': 84, 'Mall': 85, 'MARS': 86, 'AffectNet': 87, 'Penn Treebank': 88, 'OTB': 89, 'Breakfast': 90, 'HyperLex': 91, 'SearchQA': 92, 'Universal Dependencies': 93, 'SUNCG': 94, 'ELI5': 95, 'WikiSum': 96, 'KTH': 97, 'MovieQA': 98, 'COFW': 99, 'WebQuestions': 100, 'Market-1501': 101, 'LaSOT': 102, 'VOT2017': 103, 'SEMAINE': 104, 'Oxford5k': 105, 'Adience': 106, '3DPW': 107, 'MORPH': 108, 'MMI': 109, 'JAFFE': 110, 'FBMS': 111, 'FLIC': 112, 'Places': 113, 'SNIPS': 114, 'DBLP': 115, 'nuScenes': 116, 'ICDAR 2013': 117, 'LSP': 118, 'VehicleID': 119, 'CUHK03': 120, 'CLEVR': 121, 'Epinions': 122, 'DAVIS 2016': 123, 'CASIA-WebFace': 124, 'MegaFace': 125, 'DailyDialog': 126, 'CK+': 127, 'Citeseer': 128, 'USPS': 129, 'MPQA Opinion Corpus': 130, 'SALICON': 131, 'GTEA': 132, 'VoxCeleb2': 133, 'LUNA': 134, 'WikiQA': 135, 'DICM': 136, 'DukeMTMC-reID': 137, 'Letter': 138, 'Poser': 139, 'Wireframe': 140, 'SBD': 141, 'SICK': 142, 'USF': 143, 'BC5CDR': 144, 'FewRel': 145, 'FFHQ': 146, 'WikiTableQuestions': 147, 'Charades': 148, 'ScanNet': 149, 'HIC': 150, 'aPY': 151, 'Polyvore': 152, 'PETA': 153, 'ObjectNet': 154, 'WikiHow': 155, 'MSRC-12': 156, 'LFSD': 157, 'HolStep': 158, 'ENZYMES': 159, 'PROTEINS': 160, 'Matterport3D': 161, 'ConceptNet': 162, 'Pinterest': 163, 'LSUN': 164, 'FB15k': 165, 'Foursquare': 166, 'SentEval': 167, 'JHMDB': 168, 'MSVD': 169, 'LSMDC': 170, 'MELD': 171, '300W': 172, 'AwA': 173, 'VOT2016': 174, 'PartNet': 175, 'YouTube-8M': 176, 'NarrativeQA': 177, 'VOT2018': 178, 'Reddit': 179, 'CheXpert': 180, 'RESISC45': 181, 'LIP': 182, 'BookCorpus': 183, 'NABirds': 184, 'DAVIS 2017': 185, 'GTA5': 186, 'RAP': 187, 'FaceWarehouse': 188, 'iNaturalist': 189, 'E2E': 190, 'MultiWOZ': 191, 'BP4D': 192, 'CINIC-10': 193, 'LRW': 194, 'KP20k': 195, 'Ciao': 196, 'Chairs': 197, 'Cholec80': 198, 'OpenBookQA': 199, 'SWAG': 200, 'CommonsenseQA': 201, 'HRF': 202, 'PPMI': 203, 'Food-101': 204, 'CityPersons': 205, 'SUN360': 206, 'How2': 207, 'MemeTracker': 208, 'FER2013': 209, 'MCTest': 210, 'MCScript': 211, 'LVIS': 212, 'FEVER': 213, 'SFEW': 214, 'FER+': 215, 'DUTS': 216, 'PoseTrack': 217, 'WikiLarge': 218, 'AVA': 219, 'CADP': 220, 'DDI': 221, 'DISFA': 222, 'TotalCapture': 223, 'VizWiz': 224, 'AMASS': 225, 'Event2Mind': 226, 'BioASQ': 227, 'NLPR': 228, 'AOLP': 229, 'SceneNN': 230, 'NCLT': 231, 'LRS3-TED': 232, 'HOList': 233, 'SHREC': 234, 'UCY': 235, 'PanoContext': 236, 'Arcade Learning Environment': 237, 'FMA': 238, 'CHiME-5': 239, 'WikiSQL': 240, 'FG-NET': 241, 'MOTChallenge': 242, 'LOCATA': 243, 'GYAFC': 244, 'WiderPerson': 245, 'UBIRIS.v2': 246, 'TrackingNet': 247, 'COLLAB': 248, 'HotpotQA': 249, 'ISTD': 250, 'AVD': 251, 'SUN3D': 252, 'PASCAL3D+': 253, 'WebVision': 254, 'CrowdHuman': 255, 'Visual7W': 256, 'LaMem': 257, 'iKala': 258, 'CBT': 259, 'CARPK': 260, 'Sketch': 261, 'Urban100': 262, 'R2R': 263, 'UTKFace': 264, 'LRS2': 265, 'MSRDailyActivity3D': 266, 'HICO': 267, 'WebNLG': 268, 'LOL': 269, 'EmoryNLP': 270, 'DiscoFuse': 271, 'VeRi-776': 272, 'EuroSAT': 273, 'BigEarthNet': 274, 'DIRHA': 275, 'CULane': 276, 'GoPro': 277, 'Kvasir': 278, 'AMiner': 279, 'MusicNet': 280, 'LAMBADA': 281, 'Scan2CAD': 282, 'DuReader': 283, 'CoLA': 284, 'CodeSearchNet': 285, 'NEWSROOM': 286, 'WikiText-2': 287, 'THCHS-30': 288, 'SEED': 289, 'AVSD': 290, 'DiDeMo': 291, 'SIXray': 292, 'BIOSSES': 293, 'Volleyball': 294, 'EgoHands': 295, 'ConvAI2': 296, 'WikiHop': 297, 'Tatoeba': 298, 'Set11': 299, 'M4': 300, 'Set12': 301, 'DocRED': 302, 'CACD': 303, 'MUSE': 304, 'TDIUC': 305, 'LibriSpeech': 306, 'EYEDIAP': 307, 'OmniArt': 308, 'Dayton': 309, 'DeepScores': 310, '4DFAB': 311, 'Semantic3D': 312, 'WFLW': 313, 'AI2D': 314, 'MAFL': 315, 'Sprites': 316, 'VCTK': 317, 'MOT16': 318, 'MOT15': 319, 'AIDS': 320, 'Argoverse': 321, 'EgoGesture': 322, 'MegaDepth': 323, 'MultiTHUMOS': 324, 'YAGO': 325, 'Pubmed': 326, 'Panlex': 327, 'NSynth': 328, 'NYUv2': 329, 'MetaQA': 330, 'GOT-10k': 331, 'DOTA': 332, 'QUASAR-T': 333, 'HoME': 334, 'QASC': 335, 'SciERC': 336, 'BUFF': 337, 'Florence': 338, 'BeerAdvocate': 339, 'WikiConv': 340, 'WikiReading': 341, 'MAESTRO': 342, 'iPinYou': 343, 'HappyDB': 344, 'EVALution': 345, 'MLDoc': 346, 'ASTD': 347, 'RotoWire': 348, 'ITOP': 349, 'AI2-THOR': 350, 'IQUAD': 351, 'Mindboggle': 352, 'PGM': 353, 'EMOTIC': 354, '3DMatch': 355, 'VQG': 356, 'Gowalla': 357, 'TVQA': 358, 'YouCook2': 359, 'PISC': 360, 'PCam': 361, 'CORe50': 362, 'DomainNet': 363, 'OpenEDS': 364, 'XNLI': 365, 'DROP': 366, 'OpenSubtitles': 367, 'Raider': 368, 'Manga109': 369, 'WikiMovies': 370, 'FreiHAND': 371, 'xView': 372, 'NAB': 373, 'SUN09': 374, 'MOT17': 375, 'DVQA': 376, 'DIV2K': 377, 'MuPoTS-3D': 378, 'G3D': 379, 'NCI1': 380, 'Human3.6M': 381, 'CATER': 382, 'WikiArt': 383, 'VisDA-2017': 384, 'SQA': 385, 'Florentine': 386, 'SCAN': 387, 'Sim10k': 388, 'RoboCup': 389, 'OCHuman': 390, 'AQA-7': 391, 'CHASE_DB1': 392, 'Comic2k': 393, 'FERG': 394, 'VATEX': 395, 'MINC': 396, 'XSum': 397, 'MTNT': 398, 'DeepLoc': 399, 'COWC': 400, 'EmotionLines': 401, 'FC100': 402, 'SciCite': 403, 'SKU110K': 404, 'fMoW': 405, 'iSUN': 406, 'CrowdPose': 407, 'CIHP': 408, 'ScribbleSup': 409, 'ATOMIC': 410, 'CliCR': 411, 'COIN': 412, 'LIAR': 413, 'OLID': 414, 'TrajNet': 415, '3DFAW': 416, 'VegFru': 417, 'Oxford105k': 418, 'VIST': 419, 'HindEnCorp': 420, 'DREAM': 421, 'AIRS': 422, 'FSS-1000': 423, 'TGIF': 424, 'ListOps': 425, 'AISHELL-1': 426, 'WMCA': 427, 'RSICD': 428, 'VOT2014': 429, 'LSHTC': 430, 'JFT-300M': 431, 'AADB': 432, 'WinoBias': 433, 'FDDB': 434, 'ROPES': 435, 'PeerRead': 436, 'ADE20K': 437, 'decaNLP': 438, 'CQR': 439, 'JW300': 440, 'RecipeQA': 441, 'MURA': 442, 'A2D': 443, 'CAL500': 444, 'MineRL': 445, 'CrowdFlow': 446, 'CONVERSE': 447, 'EMBER': 448, 'PIRM': 449, 'SALSA': 450, 'CLINC150': 451, 'TableBank': 452, 'K2HPD': 453, 'Sports-1M': 454, 'PAWS': 455, 'WildDash': 456, 'Video2GIF': 457, 'FigureQA': 458, 'PHM2017': 459, 'ContactDB': 460}\n",
      "461\n"
     ]
    }
   ],
   "source": [
    "dataset_to_label = {} \n",
    "label_counter = 0\n",
    "\n",
    "for datasets in df_graph['positives']:\n",
    "    for dataset in datasets:\n",
    "        if dataset not in dataset_to_label:\n",
    "            dataset_to_label[dataset] = label_counter\n",
    "            label_counter += 1\n",
    "\n",
    "print(dataset_to_label)\n",
    "print(label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dataset_target_id dataset_name  \\\n",
      "0                    0     CIFAR-10   \n",
      "1                    1     ImageNet   \n",
      "2                    2      CAT2000   \n",
      "3                    3   Hopkins155   \n",
      "4                    4          VRD   \n",
      "..                 ...          ...   \n",
      "456                456     WildDash   \n",
      "457                457    Video2GIF   \n",
      "458                458     FigureQA   \n",
      "459                459      PHM2017   \n",
      "460                460    ContactDB   \n",
      "\n",
      "                                       dataset_content  \n",
      "0    The **CIFAR-10** dataset (Canadian Institute f...  \n",
      "1    The **ImageNet** dataset contains 14,197,122 a...  \n",
      "2    Includes 4000 images; 200 from each of 20 cate...  \n",
      "3    The Hopkins 155 dataset consists of 156 video ...  \n",
      "4    The Visual Relationship Dataset (**VRD**) cont...  \n",
      "..                                                 ...  \n",
      "456  WildDash is a benchmark evaluation method is p...  \n",
      "457  The **Video2GIF** dataset contains over 100,00...  \n",
      "458  FigureQA is a visual reasoning corpus of over ...  \n",
      "459  PHM2017 is a new dataset consisting of 7,192 E...  \n",
      "460  **ContactDB** is a dataset of contact maps for...  \n",
      "\n",
      "[461 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_idcontent_all = []\n",
    "\n",
    "for _, row in df_dataset_information.iterrows():\n",
    "    dataset_name = row['id']\n",
    "    dataset_content = row['contents']\n",
    "    if dataset_name in dataset_to_label.keys():\n",
    "        dataset_target_id = dataset_to_label[dataset_name]\n",
    "        dataset_idcontent_all.append((dataset_target_id, dataset_name, dataset_content))\n",
    "\n",
    "df_dataset_idcontent = pd.DataFrame(dataset_idcontent_all, columns=['dataset_target_id', 'dataset_name', 'dataset_content'])\n",
    "\n",
    "df_dataset_idcontent.sort_values(by='dataset_target_id', inplace=True, ignore_index=True)\n",
    "df_dataset_idcontent.drop_duplicates(subset=['dataset_target_id'], inplace=True, ignore_index=True)\n",
    "print(df_dataset_idcontent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_dataset = (\n",
    "    r'\\\\\\(.*?\\\\\\)|'  # LaTeX inline math (\\(...\\))\n",
    "    r'\\r\\n|'  # Line breaks\n",
    "    r'\\*\\*|'  # Asterisks\n",
    "    r'\\*|'  # Asterisks\n",
    "    r'\\$.*?\\$|'  # LaTeX inline math ($...$)\n",
    "    r'\\\\\\[.*?\\\\\\]|'  # LaTeX display math (\\[...\\])\n",
    "    r'\\\\[a-zA-Z]+\\*?(?:\\[[^\\]]*\\])?(?:\\{[^}]*\\})?|' # LaTeX commands\n",
    "    r'\\n|'\n",
    "    r'\\n+|'\n",
    "\n",
    "    r'\\(https?://\\S+\\)|'  # URLs starting with http:// or https://\n",
    "    r'(Source:|Image Source:|Image:|NOTE: ).*'\n",
    "    \n",
    ")  \n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(pattern_dataset, '', text)\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    return cleaned\n",
    "\n",
    "df_dataset_idcontent['dataset_content'] = df_dataset_idcontent['dataset_content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Textual_Feature:\n",
    "    def __init__(self, checkpoint= 'allenai/scibert_scivocab_uncased'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    def encode(self, paper_text):\n",
    "        tok_text = self.tokenizer(paper_text,\n",
    "                             truncation = True,\n",
    "                             max_length = 512, \n",
    "                             padding = 'max_length',\n",
    "                             return_tensors='pt')\n",
    "        \n",
    "        if 'token_type_ids' in tok_text:\n",
    "            del tok_text['token_type_ids']\n",
    "        return tok_text['input_ids'], tok_text['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1912849/2922747940.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_paper['abstract'] =  df_paper['abstract'].str.lower()\n",
      "/tmp/ipykernel_1912849/2922747940.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_paper['title'] =  df_paper['title'].str.lower()\n",
      "100%|██████████| 461/461 [00:00<00:00, 4236.31it/s]\n",
      "100%|██████████| 17397/17397 [00:06<00:00, 2680.40it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "textual_feature = Textual_Feature()\n",
    "df_paper['abstract'] =  df_paper['abstract'].str.lower()\n",
    "df_paper['title'] =  df_paper['title'].str.lower()\n",
    "paper_texts = df_paper['title'] + ' ' + df_paper['abstract']\n",
    "\n",
    "dataset_text = df_dataset_idcontent['dataset_content']\n",
    "\n",
    "\n",
    "token_dataset = dataset_text.progress_apply(lambda text: textual_feature.encode(text))\n",
    "token_paper = paper_texts.progress_apply(lambda text: textual_feature.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for dataset'''\n",
    "input_ids_list_dataset = [result[0] for result in token_dataset]\n",
    "attention_mask_list_dataset = [result[1] for result in token_dataset]\n",
    "\n",
    "input_ids_tensor_dataset = torch.cat(input_ids_list_dataset, dim=0)\n",
    "attention_mask_tensor_dataset = torch.cat(attention_mask_list_dataset, dim=0)\n",
    "\n",
    "'''for paper'''\n",
    "\n",
    "input_ids_list_paper = [result[0] for result in token_paper]\n",
    "attention_mask_list_paper = [result[1] for result in token_paper]\n",
    "\n",
    "input_ids_tensor_paper = torch.cat(input_ids_list_paper, dim=0)\n",
    "attention_mask_tensor_paper = torch.cat(attention_mask_list_paper, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './datafinder/train_structural.txt'\n",
    "output_csv = './datafinder/train.csv'\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        items = split(' ', line.strip())\n",
    "        paper_id = int(items[0])\n",
    "        dataset_id = int(items[1])\n",
    "        weight = items[2]\n",
    "        input_ids_p = input_ids_tensor_paper[paper_id]\n",
    "        attention_mask_p = attention_mask_tensor_paper[paper_id]\n",
    "        input_ids_d = input_ids_tensor_dataset[dataset_id]\n",
    "        attention_mask_d = attention_mask_tensor_dataset[dataset_id]\n",
    "\n",
    "        row = {\n",
    "            'paper_id': paper_id,\n",
    "            'dataset_id': dataset_id,\n",
    "            'weight': float(weight),\n",
    "            'input_ids_p': f'input_ids_p: {input_ids_p}',\n",
    "            'attention_mask_p': f'attention_mask_p: {attention_mask_p}',\n",
    "            'input_ids_d': f'input_ids_d: {input_ids_d}',\n",
    "            'attention_mask_d': f'attention_mask_d: {attention_mask_d}'\n",
    "        }\n",
    "\n",
    "        data.append(row)\n",
    "\n",
    "with open(output_csv, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['paper_id', 'dataset_id', 'weight', 'input_ids_p', 'attention_mask_p', 'input_ids_d', 'attention_mask_d']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './datafinder/test_structural.txt'\n",
    "output_csv = './datafinder/test.csv'\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    data = []\n",
    "    for line in f:\n",
    "        items = split(' ', line.strip())\n",
    "        paper_id = int(items[0])\n",
    "        dataset_id = int(items[1])\n",
    "        weight = items[2]\n",
    "        input_ids_p = input_ids_tensor_paper[paper_id]\n",
    "        attention_mask_p = attention_mask_tensor_paper[paper_id]\n",
    "        input_ids_d = input_ids_tensor_dataset[dataset_id]\n",
    "        attention_mask_d = attention_mask_tensor_dataset[dataset_id]\n",
    "\n",
    "        row = {\n",
    "            'paper_id': paper_id,\n",
    "            'dataset_id': dataset_id,\n",
    "            'weight': float(weight),\n",
    "            'input_ids_p': f'input_ids_p: {input_ids_p}',\n",
    "            'attention_mask_p': f'attention_mask_p: {attention_mask_p}',\n",
    "            'input_ids_d': f'input_ids_d: {input_ids_d}',\n",
    "            'attention_mask_d': f'attention_mask_d: {attention_mask_d}'\n",
    "        }\n",
    "\n",
    "        data.append(row)\n",
    "\n",
    "with open(output_csv, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['paper_id', 'dataset_id', 'weight', 'input_ids_p', 'attention_mask_p', 'input_ids_d', 'attention_mask_d']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
